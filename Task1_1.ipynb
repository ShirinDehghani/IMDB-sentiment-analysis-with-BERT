{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task1-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MROQnNa_n9-L",
        "outputId": "371eb8cd-f5df-41e1-b730-fd286e7d0ebc"
      },
      "source": [
        "! pip install colour\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "import sklearn\n",
        "import os\n",
        "from colour import Color\n",
        "from matplotlib import dates as mpl_dates\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM, Bidirectional\n",
        "from sklearn import metrics\n",
        "\n",
        "color = sns.color_palette()\n",
        "file_path = \"/content/drive/MyDrive/\"\n",
        "def load_data(file_path, file_name):\n",
        "  df = pd.read_csv(file_path+file_name, parse_dates=[\"date\"])\n",
        "  #df = df[df['DATA']> start]\n",
        "  df.head()\n",
        "  return df\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "maxlag=3\n",
        "\n",
        "test = 'ssr-chi2test'\n",
        "\n",
        "def grangers_causality_matrix(data, variables, test = 'ssr_chi2test', verbose=False):\n",
        "\n",
        "    dataset = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
        "\n",
        "    for c in dataset.columns:\n",
        "        for r in dataset.index:\n",
        "            test_result = grangercausalitytests(data[[r,c]], maxlag=maxlag, verbose=False)\n",
        "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
        "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
        "\n",
        "            min_p_value = np.min(p_values)\n",
        "            dataset.loc[r,c] = min_p_value\n",
        "\n",
        "    dataset.columns = [var + '_x' for var in variables]\n",
        "\n",
        "    dataset.index = [var + '_y' for var in variables]\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def grangers_causality_vector(data, variables,target = 'close', test = 'ssr_chi2test', verbose=False):\n",
        "\n",
        "    dataset = pd.DataFrame(columns=variables)\n",
        "    datas = []\n",
        "    for c in variables:\n",
        "      test_result = grangercausalitytests(data[[target,c]].dropna(), maxlag=maxlag, verbose=False)\n",
        "      p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
        "      if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
        "      min_p_value = np.min(p_values)\n",
        "      dataset.loc[0,c] = min_p_value\n",
        "    # for c in dataset.columns:\n",
        "    #     for r in dataset.index:\n",
        "    #         test_result = grangercausalitytests(data[[r,c]], maxlag=maxlag, verbose=False)\n",
        "    #         p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
        "    #         if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
        "\n",
        "    #         min_p_value = np.min(p_values)\n",
        "    #         dataset.loc[r,c] = min_p_value\n",
        "\n",
        "    # dataset.columns = [var + '_x' for var in variables]\n",
        "\n",
        "    # dataset.index = [var + '_y' for var in variables]\n",
        "\n",
        "    return dataset\n",
        "def calc_corr(df, thr,do_plot=1):\n",
        "  corrmat = df.corr(method='pearson')\n",
        "  # fig, ax = plt.subplots(figsize=(20, 20))\n",
        "  \n",
        "  red = Color(\"red\")\n",
        "  colors = list(red.range_to(Color(\"green\"),len(df.columns)))\n",
        "  colors = [color.rgb for color in colors]\n",
        "\n",
        "  corrmat = corrmat.sort_values(by=['close'])\n",
        "  # corrmat['close'].plot.barh(color=colors)\n",
        "  #sns.heatmap(corrmat, vmax=1., square=False)\n",
        "  corrmat = corrmat[(corrmat['close'] > thr)]\n",
        "  selected_features = corrmat.index\n",
        "  if do_plot:\n",
        "    plt.grid(True, which='both', axis='x')\n",
        "    plt.title(\"Cryptocurrency correlation map\", fontsize=15)\n",
        "    plt.savefig(\"correlation map\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "  return list(selected_features)\n",
        "def split_data(df, seq_size, selected_features, target):\n",
        "    num_of_slices = int(len(df) / seq_size)\n",
        "    \n",
        "    X = list()\n",
        "    Y = list()\n",
        "    for i in range(0,len(df)-seq_size):\n",
        "        sequence = df.iloc[i:i+seq_size - 1][selected_features].values\n",
        "        # if len(X) == 0:\n",
        "        #   X = sequence\n",
        "        #   Y = df.iloc[i+seq_size][target]\n",
        "        # else:\n",
        "        X.append(sequence)\n",
        "        Y.append(df.iloc[i+seq_size][target]) \n",
        "        # if len(X) == 0:\n",
        "        #     X = sequence\n",
        "        # else:\n",
        "        #     X= \n",
        "        #     #rms_res = np.concatenate((rms_res, rms_window), axis=0)\n",
        "        # trg.append(rms[i + window_size])\n",
        "        #     #rul.append(int((1 - (((j * size)+i)/life))*10))\n",
        "    return X, Y\n",
        "\n",
        "def normalize(X, Y,normalise_type=0):\n",
        "  X_n = [[[0 for k in range(len(X[0][0]))] for j in range(len(X[0]))] for i in range(len(X))]\n",
        "  Y_n = [0 for k in range(len(Y))]\n",
        "  inverse_transform = [0 for k in range(len(Y))]\n",
        "  for i in range(0, len(X)): \n",
        "    for j in range(0, len(X[0])): \n",
        "      for k in range (0, len(X[0][0])): \n",
        "        # if 'GDELT' in selected_features[k]:\n",
        "        #   X_n[i][j][k] = X[i][j][k]\n",
        "        # else:\n",
        "        if normalise_type == 0:\n",
        "          if X[i][0][k] != 0:\n",
        "            X_n[i][j][k] = (X[i][j][k] / X[i][0][k]) - 1 \n",
        "        if normalise_type == 1:\n",
        "          if X[i][j-1][k] !=0:\n",
        "            X_n[i][j][k] = (X[i][j][k] / X[i][j-1][k]) - 1 \n",
        "\n",
        "  return X_n\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM, Bidirectional\n",
        "from sklearn import metrics\n",
        "def LSTM_net(X, Y, in_shape, ep=200, val=0.3, ver=2):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(10,return_sequences=True, input_shape=in_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(10,return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(10,return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(10,return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "   \n",
        "    \n",
        "\n",
        "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch / 20))\n",
        "    opt = tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=opt,\n",
        "              metrics=[\"mse\"])\n",
        "    \n",
        "    #model.compile(optimizer='adam', loss='mse')\n",
        "    history = model.fit(X,Y, epochs=ep, validation_split=val, verbose=ver,callbacks=[lr_schedule])\n",
        "    \n",
        "    # print(history.history[\"loss\"])\n",
        "    # plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "    # plt.axis([1e-2, 0, 0, 0.1])   \n",
        "    # plt.show()\n",
        "    # plt.close()\n",
        "    ind = np.argmin(history.history[\"loss\"])\n",
        "    \n",
        "    LR = history.history[\"lr\"][ind]\n",
        "    LR= 0.001\n",
        "    # print('Learning_rate: ' , LR)\n",
        "    model.compile(loss='mse', optimizer = tf.keras.optimizers.Adam(learning_rate= LR))#tf.keras.optimizers.RMSprop())\n",
        "    history = model.fit(X, Y, epochs=ep, validation_split=val, verbose=ver, shuffle=False)\n",
        "\n",
        "    \n",
        "    return model\n",
        "    \n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "# df = load_data(file_path, \"data_set.csv\")\n",
        "# df = load_data(file_path, \"new_dataset.csv\")\n",
        "def risk_sensitive_func(x,a,mode,b=10):\n",
        "  if mode == 'Pragmatism':\n",
        "    y = 1-np.exp(-a*abs(x-1))\n",
        "    return y\n",
        "  if mode == 'Threshold':\n",
        "    y = 1-np.exp(-a*abs(x-1))\n",
        "    if y>0.5:\n",
        "      k = 1\n",
        "    else:\n",
        "      k = 0\n",
        "    return k\n",
        "  if mode == 'RiskLevel':\n",
        "    a1 = a\n",
        "    a2 = b/a\n",
        "    if x>=1:\n",
        "      y = 1-np.exp(-a1*abs(x-1))\n",
        "    if x<1:\n",
        "      y = 1-np.exp(-a2*abs(x-1))\n",
        "    return y\n",
        "  if mode == 'Thresholdrisk':\n",
        "    a1 = a\n",
        "    a2 = b/a\n",
        "    if x>=1:\n",
        "      y = 1-np.exp(-a1*abs(x-1))\n",
        "    if x<1:\n",
        "      y = 1-np.exp(-a2*abs(x-1))\n",
        "    if y>0.5:\n",
        "      k = 1\n",
        "    else:\n",
        "      k = 0\n",
        "    return k\n",
        "def get_risk_sensitive_params(mode):\n",
        "  if mode == 'Pragmatism':\n",
        "    y = [[5],[10],[40],[80],[160]]\n",
        "    return y\n",
        "  if mode == 'Threshold':\n",
        "    y = [[5],[10],[40],[80],[160]]\n",
        "    return y\n",
        "  if mode == 'RiskLevel':\n",
        "    y = [[1,80],[10,80],[20,80],[40,80],[60,80],[80,80]]\n",
        "    # b = [10,10,10,40,40,40]\n",
        "    return y\n",
        "  if mode == 'Thresholdrisk':\n",
        "    y = [[1,80],[10,80],[20,80],[40,80],[60,80],[80,80]]\n",
        "    # b = [10,10,10,40,40,40]\n",
        "    return y\n",
        "\n",
        "#@title Control Panel\n",
        "dataset_type = \"hourly\" #@param [\"daily\", \"hourly\"]\n",
        "use_loss_limit = False #@param {type:\"boolean\"}\n",
        "profiting_interval = False #@param {type:\"boolean\"}\n",
        "crypto_type = \"eth\" #@param [\"btc\", \"ltc\", \"bnb\", \"eth\"]\n",
        "observe_delay = \"_2\" #@param [\"\", \"_2\", \"_3\", \"_6\",\"_12\"] {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colour\n",
            "  Downloading colour-0.1.5-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: colour\n",
            "Successfully installed colour-0.1.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xBX9L2pMeJ6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J87kT0ylohZB"
      },
      "source": [
        "# Task1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAoyq2hRolM-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "5cc258d1-2861-45cb-8034-cbaa097e991b"
      },
      "source": [
        "import warnings\n",
        "import re,os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "import datetime as dt\n",
        "import dateutil.relativedelta as relativedelta\n",
        "from datetime import datetime\n",
        "import re\n",
        "from scipy.signal import argrelextrema\n",
        "warnings.filterwarnings('ignore')\n",
        "# #@title Control Panel\n",
        "# dataset_type = \"daily\" #@param [\"daily\", \"hourly\"]\n",
        "# use_loss_limit = False #@param {type:\"boolean\"}\n",
        "# profiting_interval = False #@param {type:\"boolean\"}\n",
        "# crypto_type = \"btc\" #@param [\"btc\", \"ltc\", \"bnb\", \"eth\"]\n",
        "# observe_delay = \"_2\" #@param [\"\", \"_2\", \"_3\", \"_6\",\"_12\"] {type:\"string\"}\n",
        "round_cnt = 0\n",
        "do_the_new_idea = False\n",
        "redu_prev_trades = False\n",
        "redu_previous_preds = False\n",
        "ignore_existense_trades = True\n",
        "dont_sell_inside_candle = True\n",
        "Transaction_cost_rate = 0.001\n",
        "# dont_sell_inside_candle = False\n",
        "# for crypto_type in [\"btc\"]: #[\"btc\", \"ltc\", \"eth\"]:\n",
        "for crypto_type in [\"eth\"]:\n",
        "  for mode_dataset_strip in ['remove_first_month','use_all_from_begin']:\n",
        "    for dataset_type in ['daily','hourly']:\n",
        "    # for dataset_type in ['hourly']:\n",
        "      \n",
        "      if dataset_type == 'daily':\n",
        "        list_options = [\"\", \"_2\", \"_3\"]\n",
        "        list_options = [\"\"]\n",
        "      if dataset_type == 'hourly':\n",
        "        list_options = [\"\",\"_3\" ,\"_6\", \"_12\"]\n",
        "        list_options = [\"_3\" ,\"_6\", \"_12\"]\n",
        "        # list_options = [\"\"]\n",
        "\n",
        "      for observe_delay in list_options:\n",
        "        dataset_name = 'Crypto_Datasets_shared/'+crypto_type+'_'+dataset_type+str(observe_delay)\n",
        "          \n",
        "        round_cnt+=1\n",
        "        print('round ',round_cnt)\n",
        "        print('mode_dataset_strip ' ,mode_dataset_strip)\n",
        "        print('dataset_type ',dataset_type)\n",
        "        print('observe_delay ',observe_delay)\n",
        "        # if not profiting_interval:\n",
        "        #   end_time = '2021-07-01 00:00:00'\n",
        "        # if profiting_interval:\n",
        "        #   end_time = '2020-12-31 00:00:00'\n",
        "        file1 = '/content/drive/MyDrive/long_term_simulation_with_loss_bound_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_'+mode_dataset_strip+'_task_1'+'.csv'\n",
        "        file2 = '/content/drive/MyDrive/long_term_simulation_without_loss_bound_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_'+mode_dataset_strip+'_task_1'+'.csv'\n",
        "        do_prediction_in_this_loop = False\n",
        "        do_trades_in_this_loop = False\n",
        "        if os.path.exists(file1) and os.path.exists(file2) and not ignore_existense_trades:\n",
        "          print('skipping as it has already runned completely')\n",
        "          if redu_prev_trades:\n",
        "              do_trades_in_this_loop = True\n",
        "        else:\n",
        "          do_trades_in_this_loop = True\n",
        "          if not redu_previous_preds and  os.path.exists('/content/drive/MyDrive/inference_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_'+mode_dataset_strip+'_task_1'+'.csv'):\n",
        "              print('resume incomplete process')\n",
        "              do_prediction_in_this_loop = True\n",
        "              df_predictions_hourly = pd.read_csv('/content/drive/MyDrive/inference_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_'+mode_dataset_strip+'_task_1'+'.csv')     \n",
        "              \n",
        "              if dataset_type == 'daily':\n",
        "                  df_predictions_hourly['date']= pd.to_datetime(df_predictions_hourly['date'],format='%Y-%m-%d', errors='coerce').dt.date\n",
        "              else:\n",
        "                  df_predictions_hourly['date']= pd.to_datetime(df_predictions_hourly['date'],format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "              last_test_date = df_predictions_hourly['date'][len(df_predictions_hourly)-1] - pd.DateOffset(days=round(1))\n",
        "              end_train_time = str(last_test_date)\n",
        "              print('end_train_time = ',str(last_test_date))\n",
        "              ender_D  = df_predictions_hourly['date'][len(df_predictions_hourly)-1]  + pd.DateOffset(days=round(30))\n",
        "              ender_time = str(ender_D)\n",
        "              print('ender_time = ',str(ender_D))\n",
        "              list_time_stomps_lstm = df_predictions_hourly['date'].tolist()\n",
        "              list_time_stomps_lstm_attention = []\n",
        "              list_perdictions_lstm = list(df_predictions_hourly['Estimated_change_ratio_LSTM'])\n",
        "              list_perdictions_lstm_attention = []\n",
        "              \n",
        "          else:\n",
        "              do_prediction_in_this_loop = True\n",
        "              end_train_time = '2018-06-30 00:00:00'\n",
        "              ender_time = '2018-08-01 00:00:00'\n",
        "              df_predictions_hourly = pd.DataFrame()\n",
        "              list_time_stomps_lstm = []\n",
        "              list_time_stomps_lstm_attention = []\n",
        "              list_perdictions_lstm = []\n",
        "              list_perdictions_lstm_attention = []\n",
        "        \n",
        "        \n",
        "          \n",
        "          if do_prediction_in_this_loop:\n",
        "            print('starting the predictions')\n",
        "            dd = datetime.now()\n",
        "            created_date = str(dd.year)+'-'+str(dd.month)+'-'+str(dd.day)+' -H'+str(dd.hour)\n",
        "            df = load_data(file_path, dataset_name+\".csv\")\n",
        "            # rd = relativedelta.relativedelta(day = 6)\n",
        "            start_time = str(df.loc[0,'date'])\n",
        "    \n",
        "            # start_time = '2017-01-01 00:00:00'\n",
        "            '''\n",
        "            if os.path.exists('/content/drive/MyDrive/inference_hourly.csv'):\n",
        "              df_predictions_hourly = pd.read_csv('/content/drive/MyDrive/inference_hourly.csv')   \n",
        "              start_time = df_predictions_hourly.loc[len(df_predictions_hourly)-1,'date']\n",
        "            '''\n",
        "            end_time = str(df.loc[len(df)-1,'date'])\n",
        "            # end_time = '2021-08-08 00:00:00'\n",
        "            text = start_time\n",
        "            if dataset_type == 'daily':\n",
        "              match = re.search(r'\\d{4}-\\d{2}-\\d{2}', text)\n",
        "            else:\n",
        "              match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', text)\n",
        "            if dataset_type == 'daily':\n",
        "              start_date = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
        "            else:\n",
        "              start_date = datetime.strptime(match.group(), '%Y-%m-%d %H:%M:%S')\n",
        "    \n",
        "            text = end_train_time\n",
        "            if dataset_type == 'daily':\n",
        "              match = re.search(r'\\d{4}-\\d{2}-\\d{2}', text)\n",
        "            else:\n",
        "              match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', text)\n",
        "            if dataset_type == 'daily':\n",
        "              end_train_date = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
        "            else:\n",
        "              end_train_date = datetime.strptime(match.group(), '%Y-%m-%d %H:%M:%S')\n",
        "    \n",
        "            text = end_time\n",
        "            if dataset_type == 'daily':\n",
        "              match = re.search(r'\\d{4}-\\d{2}-\\d{2}', text)\n",
        "            else:\n",
        "              match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', text)\n",
        "            # =========== if match is not None:\n",
        "            if dataset_type == 'daily':\n",
        "              end_date = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
        "            else:\n",
        "              end_date = datetime.strptime(match.group(), '%Y-%m-%d %H:%M:%S')\n",
        "            # =========== if match is not None:\n",
        "            # start_date2 = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
        "            text = ender_time\n",
        "            if dataset_type == 'daily':\n",
        "              match = re.search(r'\\d{4}-\\d{2}-\\d{2}', text)\n",
        "            else:\n",
        "              match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', text)\n",
        "            # =========== if match is not None:\n",
        "            if dataset_type == 'daily':\n",
        "              ender_date = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
        "            else:\n",
        "              ender_date = datetime.strptime(match.group(), '%Y-%m-%d %H:%M:%S')\n",
        "            # start_date2 =ender_date-pd.DateOffset(days=round(562))\n",
        "            if mode_dataset_strip == 'remove_first_month':\n",
        "              start_date2 =ender_date-pd.DateOffset(days=round(375))\n",
        "            else:\n",
        "              start_date2 = start_date\n",
        "            # start_date2 =ender_date-pd.DateOffset(days=round(210))\n",
        "            # tempdate = start_date2 \n",
        "            \n",
        "            cnt = 0\n",
        "            \n",
        "            break_next_time = 0\n",
        "            num_days = 360\n",
        "            while True:\n",
        "              \n",
        "              if ender_date > end_date:\n",
        "                break\n",
        "              #print(list_time_stomps_lstm)\n",
        "              #print(list_time_stomps_lstm_attention)\n",
        "              #print(list_perdictions_lstm)\n",
        "              #print(list_perdictions_lstm_attention)\n",
        "              cnt+=1\n",
        "              \n",
        "    \n",
        "              do_mapping = False\n",
        "              from datetime import datetime\n",
        "              import re\n",
        "              from scipy.signal import argrelextrema\n",
        "              \n",
        "              if dataset_type == 'daily':\n",
        "                match = re.search(r'\\d{4}-\\d{2}-\\d{2}', str(ender_date))\n",
        "              else:\n",
        "                match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', str(ender_date))\n",
        "              # if match is not None:\n",
        "              # end_date = datetime.strptime(match.group(), \"%Y-%m-%d\")\n",
        "              # start_date.month = 11\n",
        "              df = load_data(file_path, dataset_name+\".csv\")\n",
        "              df = df.drop(columns=['symbol'])\n",
        "              # print(df['date'])\n",
        "              # self.df[\"date\"] = pd.to_datetime(self.df[\"date\"]).dt.date\n",
        "              if dataset_type == 'daily':\n",
        "                df['date']= pd.to_datetime(df['date'],format='%Y-%m-%d', errors='coerce').dt.date\n",
        "              else:\n",
        "                df['date']= pd.to_datetime(df['date'],format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "              df['close2'] = df['close']\n",
        "              data_bitcoin2015 = df.loc[(start_date2<=df['date'])&(df['date']<=ender_date)]\n",
        "              data_bitcoin2015 = data_bitcoin2015.reset_index(drop=True)\n",
        "              data_bitcoin2015 = data_bitcoin2015.drop(columns = ['Unnamed: 0'])\n",
        "              data_bitcoin2015_normalized = data_bitcoin2015.copy()\n",
        "              for col in data_bitcoin2015_normalized.columns:\n",
        "                data_bitcoin2015_normalized[col].fillna(method='ffill', inplace=True)\n",
        "              num_rows = len(data_bitcoin2015_normalized)\n",
        "              columns = list(data_bitcoin2015_normalized.columns)\n",
        "              for c in columns:\n",
        "                if 'GDELT' in c and not 'Tone' in c:\n",
        "                  data_bitcoin2015_normalized[c] = pd.to_numeric(data_bitcoin2015_normalized[c], downcast=\"float\")\n",
        "                  for ic in range(len(data_bitcoin2015_normalized)-1):\n",
        "                    data_bitcoin2015_normalized.loc[ic+1,c] = data_bitcoin2015_normalized.loc[ic+1,c]*0.1+0.9*data_bitcoin2015_normalized.loc[ic,c]\n",
        "              df = data_bitcoin2015_normalized.copy()\n",
        "    \n",
        "    \n",
        "    \n",
        "              # order: How many points on each side to use for the comparison to consider\n",
        "              ilocs_min = argrelextrema(data_bitcoin2015.close.values, np.less_equal, order=15)[0]\n",
        "              ilocs_max = argrelextrema(data_bitcoin2015.close.values, np.greater_equal, order=15)[0]\n",
        "    \n",
        "              # df.Close.plot(figsize=(20,8), alpha=.3)\n",
        "              # # filter prices that are peaks and plot them differently to be visable on the plot\n",
        "              # data_bitcoin2015.iloc[ilocs_max].Close.plot(style='.', lw=10, color='red', marker=\"v\");\n",
        "              # data_bitcoin2015.iloc[ilocs_min].Close.plot(style='.', lw=10, color='green', marker=\"^\");\n",
        "              if do_mapping:\n",
        "                num_cols = len(columns)\n",
        "                for col in range(2,num_cols,1):\n",
        "                  for row in range(1,num_rows,1):\n",
        "                    if columns[col]=='close':\n",
        "                      if data_bitcoin2015.loc[row-1, columns[col]] == 0:\n",
        "                        data_bitcoin2015_normalized.loc[row, columns[col]] = 1#data_bitcoin2015_normalized.loc[row-1, columns[col]]  \n",
        "                      else:\n",
        "                        if data_bitcoin2015.loc[row, columns[col]]/data_bitcoin2015.loc[row-1, columns[col]]>1:\n",
        "                          data_bitcoin2015_normalized.loc[row, columns[col]] = 1.5\n",
        "                        else:\n",
        "                          data_bitcoin2015_normalized.loc[row, columns[col]] = 0.5\n",
        "              else:\n",
        "                num_cols = len(columns)\n",
        "                for col in range(2,num_cols,1):\n",
        "                  for row in range(1,num_rows,1):\n",
        "                    if columns[col]=='close':\n",
        "                      if data_bitcoin2015.loc[row-1, columns[col]] == 0:\n",
        "                        data_bitcoin2015_normalized.loc[row, columns[col]] = data_bitcoin2015_normalized.loc[row-1, columns[col]]  \n",
        "                      else:\n",
        "                        data_bitcoin2015_normalized.loc[row, columns[col]] = 1+5*(data_bitcoin2015.loc[row, columns[col]]/data_bitcoin2015.loc[row-1, columns[col]]-1)\n",
        "              # data_bitcoin2015_normalized.remove('Unnamed: 0')\n",
        "              # data_bitcoin2015_normalized = data_bitcoin2015_normalized.drop(columns = ['Unnamed: 0'])\n",
        "    \n",
        "              data_bitcoin2015_normalized_rl = data_bitcoin2015.copy()\n",
        "              columns = list(data_bitcoin2015_normalized_rl.columns)\n",
        "              num_rows = len(data_bitcoin2015_normalized_rl)\n",
        "              num_cols = len(columns)\n",
        "              # for col in range(2,num_cols,1):\n",
        "              #   for row in range(2,num_rows,1):\n",
        "              #     if data_bitcoin2015.loc[row-1, columns[col]] != 0:\n",
        "              #       data_bitcoin2015_normalized_rl.loc[row, columns[col]] = data_bitcoin2015.loc[row, columns[col]]/data_bitcoin2015.loc[row-1, columns[col]]\n",
        "              #     else: \n",
        "              #       data_bitcoin2015_normalized_rl.loc[row, columns[col]] = 1\n",
        "              # data_bitcoin2015_normalized_rl = data_bitcoin2015_normalized_rl.fillna(method='ffill')\n",
        "    \n",
        "              import pickle\n",
        "              MAX_EPISODES = 3\n",
        "              import os\n",
        "              BATCH_SIZE = 32\n",
        "              num_runs = 10\n",
        "              Risk_modes = ['Thresholdrisk','Pragmatism','Threshold','RiskLevel']\n",
        "              method_list = ['LSTM'] #['LSTM_self_Attention','LSTM']\n",
        "              feature_engineering_method_list = ['Corr']#,'Granger']\n",
        "              risk_sensitive_params = [1,2,5,10,40]\n",
        "              corr_threshold_list = [0.7,0.8,0.85] #[0,0.2,0.4,0.6,0.7,0.75,0.8,0.85,9]\n",
        "              split_percentage_list = [0.84] # [0.75,0.8,0.85,0.9]\n",
        "              seq_size_list = [10]\n",
        "              normalise_list = [1]\n",
        "              days_after_to_predict_list = [1]\n",
        "              #,3,5,7]\n",
        "              max_mse = [200000,200000,200000,200000]\n",
        "              min_class_acc = [0,0,0,0]\n",
        "              has_runned_percentage_method = [[False,False,False],[False,False,False],[False,False,False],[False,False,False]]\n",
        "              DDQN_runned = [[False,False,False],[False,False,False],[False,False,False],[False,False,False]]\n",
        "              DDQN_LSTM_runned = [[False,False,False],[False,False,False],[False,False,False],[False,False,False]]\n",
        "              runned_before = 0\n",
        "              #  np.zeros((len(days_after_to_predict_list),len(split_percentage_list),len(method_list)))\n",
        "              df_results = pd.DataFrame()\n",
        "              # dataset_name = 'Ethereum dataset'\n",
        "              # df_results  = pd.read_pickle('/content/drive/MyDrive/Mofid_Ether_infer/result_'+dataset_name+'.pkl')\n",
        "              # df_results.index= [i for i in range(len(df_results))]\n",
        "              # pd.set_option('display.max_rows', len(df_results))\n",
        "              num_r = 0 \n",
        "    \n",
        "              for days_after_to_predict in days_after_to_predict_list:\n",
        "                for split_percentage in split_percentage_list:\n",
        "                  for method in method_list:   \n",
        "                    for feature_engineering_method in feature_engineering_method_list:\n",
        "                      # for corr_threshold in corr_threshold_list:\n",
        "                      for normalise_t in normalise_list:\n",
        "                        for seq_size in seq_size_list:\n",
        "                          num_r+=1\n",
        "                          if num_r>runned_before:\n",
        "                            \n",
        "                            #print('method = ',method)\n",
        "                            #print('days_after_to_predict = ',days_after_to_predict)\n",
        "                            #print('feature engineering method = ',feature_engineering_method)\n",
        "                            # print('corr threshold = ',corr_threshold)\n",
        "                            #print('split_percentage = ',split_percentage)\n",
        "                            #print('normalise_t = ',normalise_t)\n",
        "                            #print('seq_size = ',seq_size)\n",
        "                            # if not os.path.exists('/content/drive/MyDrive/Mofid_infer/'+end_time+'RS_'+risk_mode+'_'+str(risk_sensitive_param[0])+'_one_run'+str(split_percentage)+'_'+method+'_acc.pkl'):\n",
        "                            corr_threshold = 0.8\n",
        "                            if feature_engineering_method == 'Corr':\n",
        "                              cols = df.columns\n",
        "                              selected_features = []\n",
        "                              #selected_features.extend(df.columns[2:])\n",
        "                              selected_features.extend(calc_corr(df, corr_threshold,0))\n",
        "                              selected_features.remove('close')\n",
        "                              #print(selected_features)\n",
        "                              \n",
        "                              #display(df, selected_features)\n",
        "                              num_of_features = len(selected_features)\n",
        "                            if feature_engineering_method == 'Granger':\n",
        "                              gc_df = grangers_causality_vector(df.iloc[:,2:], variables = df.iloc[:,2:].columns)  \n",
        "                              selected_features = [c for c in gc_df.columns if 1>gc_df.loc[0,c]>corr_threshold]\n",
        "                              num_of_features = len(selected_features)\n",
        "                              #print(selected_features)\n",
        "    \n",
        "                            for col in data_bitcoin2015_normalized.columns:\n",
        "                              data_bitcoin2015_normalized[col].fillna(method='ffill', inplace=True)\n",
        "                            \n",
        "                            df_use = data_bitcoin2015.loc[[i for j, i in enumerate(data_bitcoin2015.index) if j % days_after_to_predict == 0]]\n",
        "                            df_use_norm = data_bitcoin2015_normalized.loc[[i for j, i in enumerate(data_bitcoin2015_normalized.index) if j % days_after_to_predict == 0]]\n",
        "                            \n",
        "                            #seq_size = input('Enter Sequence Size: ')\n",
        "                            # df.loc[(start_date2<=df['date'])&(df['date']<=ender_date)]\n",
        "                            time_to_delta = pd.DateOffset(days=round(0))\n",
        "                            if dataset_type == 'daily':                          \n",
        "                              if observe_delay == '':\n",
        "                                time_to_delta = pd.DateOffset(days=round(seq_size))\n",
        "                              if observe_delay == '_2':\n",
        "                                time_to_delta = pd.DateOffset(days=round(seq_size*2))\n",
        "                              if observe_delay == '_3':\n",
        "                                time_to_delta = pd.DateOffset(days=round(seq_size*3))\n",
        "                            if dataset_type == 'hourly':                          \n",
        "                              if observe_delay == '':\n",
        "                                time_to_delta = pd.DateOffset(hours=round(seq_size))\n",
        "                              if observe_delay == '_6':\n",
        "                                time_to_delta = pd.DateOffset(hours=round(seq_size*6))\n",
        "                              if observe_delay == '_3':\n",
        "                                time_to_delta = pd.DateOffset(hours=round(seq_size*3))\n",
        "                              if observe_delay == '_12':\n",
        "                                time_to_delta = pd.DateOffset(hours=round(seq_size*12))\n",
        "                            df_train = df_use_norm.loc[(start_date2<=df_use_norm['date'])&(df_use_norm['date']<=end_train_date)]\n",
        "                            df_test = df_use_norm.loc[(end_train_date-time_to_delta<=df_use_norm['date'])&(df_use_norm['date']<=ender_date)]\n",
        "                            df_test_real_val = df_use.loc[(end_train_date-time_to_delta<=df_use['date'])&(df_use['date']<=ender_date)]\n",
        "                            X_train, Y_train = split_data(df_train, seq_size ,1, 'close')\n",
        "                            X_train = normalize(X_train, Y_train)\n",
        "                            X_test, Y_test = split_data(df_test, seq_size ,selected_features, 'close')\n",
        "                            X_test =  normalize(X_test, Y_test)\n",
        "                            X_use ,_= split_data(df_use_norm, seq_size ,selected_features, 'close')\n",
        "                            #print(len(X_use))\n",
        "                            #print(len(df_use_norm))\n",
        "                            X_train = np.array(X_train).reshape(len(X_train), seq_size-1, num_of_features)\n",
        "                            Y_train = np.array(Y_train).reshape(len(Y_train), 1)\n",
        "                            in_shape = (seq_size-1, num_of_features)\n",
        "                            buy_sell_vec = []\n",
        "                            buy_sell_vec_intensity = []\n",
        "                            pred_use = [1 for i in range(seq_size-1)]\n",
        "                            if method == 'buy_at_first':\n",
        "                              buy_sell_vec = [1 for i in range(len(df_use))]\n",
        "                              buy_sell_vec[0] = 1\n",
        "                              buy_sell_vec_intensity = [1 for i in range(len(df_use))]\n",
        "                            if method == 'LSTM':\n",
        "                              best_acc = 0\n",
        "                              n_retry = 0\n",
        "                              best_out_test = []\n",
        "                              while n_retry <3 and (n_retry >1 or best_acc <0.55):\n",
        "                                for corr_threshold in corr_threshold_list:\n",
        "                                  if feature_engineering_method == 'Corr':\n",
        "                                    cols = df.columns\n",
        "                                    selected_features = []\n",
        "                                    #selected_features.extend(df.columns[2:])\n",
        "                                    selected_features.extend(calc_corr(df, corr_threshold,0))\n",
        "                                    selected_features.remove('close')\n",
        "                                    #print(selected_features)\n",
        "                                    \n",
        "                                    #display(df, selected_features)\n",
        "                                    num_of_features = len(selected_features)\n",
        "                                  if feature_engineering_method == 'Granger':\n",
        "                                    gc_df = grangers_causality_vector(df.iloc[:,2:], variables = df.iloc[:,2:].columns)  \n",
        "                                    selected_features = [c for c in gc_df.columns if 1>gc_df.loc[0,c]>corr_threshold]\n",
        "                                    num_of_features = len(selected_features)\n",
        "                                    #print(selected_features)\n",
        "    \n",
        "                                  for col in data_bitcoin2015_normalized.columns:\n",
        "                                    data_bitcoin2015_normalized[col].fillna(method='ffill', inplace=True)\n",
        "                                  \n",
        "                                  df_use = data_bitcoin2015.loc[[i for j, i in enumerate(data_bitcoin2015.index) if j % days_after_to_predict == 0]]\n",
        "                                  df_use_norm = data_bitcoin2015_normalized.loc[[i for j, i in enumerate(data_bitcoin2015_normalized.index) if j % days_after_to_predict == 0]]\n",
        "                                  \n",
        "                                  #seq_size = input('Enter Sequence Size: ')\n",
        "                                  # df.loc[(start_date2<=df['date'])&(df['date']<=ender_date)]\n",
        "                                  df_train = df_use_norm.loc[(start_date2<=df_use_norm['date'])&(df_use_norm['date']<=end_train_date)]\n",
        "                                  df_test = df_use_norm.loc[(end_train_date-time_to_delta<=df_use_norm['date'])&(df_use_norm['date']<=ender_date)]\n",
        "                                  df_test_real_val = df_use.loc[(end_train_date-time_to_delta<=df_use['date'])&(df_use['date']<=ender_date)]\n",
        "                                  X_train, Y_train = split_data(df_train, seq_size ,selected_features, 'close')\n",
        "                                  X_train = normalize(X_train, Y_train)\n",
        "                                  X_test, Y_test = split_data(df_test, seq_size ,selected_features, 'close')\n",
        "                                  X_test =  normalize(X_test, Y_test)\n",
        "                                  X_use ,_= split_data(df_use_norm, seq_size ,selected_features, 'close')\n",
        "                                  #print(len(X_use))\n",
        "                                  #print(len(df_use_norm))\n",
        "                                  #print(len(X_train))\n",
        "                                  #print(len(X_test))\n",
        "                                  X_train = np.array(X_train).reshape(len(X_train), seq_size-1, num_of_features)\n",
        "                                  Y_train = np.array(Y_train).reshape(len(Y_train), 1)\n",
        "                                  in_shape = (seq_size-1, num_of_features)\n",
        "                                  model = LSTM_net(X_train, Y_train, in_shape, ep =50, ver=0)\n",
        "                                  Y_pred_rel = list()\n",
        "                                  Y_real_rel = list()\n",
        "                                  for i in range(len(X_test)):\n",
        "                                    X_temp = np.array(X_test[i]).reshape(1, seq_size-1, num_of_features)\n",
        "                                    Y_temp = model.predict(X_temp)\n",
        "                                    Y_real_rel.append(Y_test[i])\n",
        "                                    Y_pred_rel.append(Y_temp[0][-1])\n",
        "                                  if np.any(np.isnan(Y_pred_rel)): \n",
        "                                    #print('y pred has nan')\n",
        "                                    Y_pred_rel = [np.nan_to_num(citem) for citem in Y_pred_rel]  \n",
        "                                  if np.any(np.isnan(Y_real_rel)):  \n",
        "                                    #print('y real has nan')\n",
        "                                    Y_real_rel = [np.nan_to_num(citem) for citem in Y_real_rel]\n",
        "                                  if True: \n",
        "                                    mserror_rel = sklearn.metrics.mean_squared_error(Y_real_rel, Y_pred_rel)\n",
        "                                    corrrr_rel = np.corrcoef(np.array(Y_real_rel).reshape((len(Y_real_rel))),np.array(Y_pred_rel).reshape((len(Y_real_rel))))[1,0]\n",
        "                                    class_acc = np.mean(((np.array(Y_real_rel)>1)&(np.array(Y_pred_rel)>1))|((np.array(Y_real_rel)<1)&(np.array(Y_pred_rel)<1)))\n",
        "                                    #print('classification_accuracy', class_acc)\n",
        "                                    ts_pred = list()\n",
        "                                    Y_pred = list()\n",
        "                                    Y_real = list()\n",
        "                                    for i in range(len(X_test)):\n",
        "                                      X_temp = np.array(X_test[i]).reshape(1, seq_size-1, num_of_features)\n",
        "                                      Y_temp = model.predict(X_temp)\n",
        "                                      if i-1+len(df_train) in range(len(data_bitcoin2015)):\n",
        "                                        Y_real.append(Y_test[i]*data_bitcoin2015.loc[i-1+len(df_train), 'close'])\n",
        "                                        Y_pred.append(Y_temp[0][-1]*data_bitcoin2015.loc[i-1+len(df_train), 'close'])\n",
        "                                        ts_pred.append(data_bitcoin2015.loc[i-1+len(df_train), 'date'])\n",
        "                                      # Y_real.append(Y_test[i]*data_bitcoin2015.loc[i-1+len(df_train), 'close'])\n",
        "                                      # Y_pred.append(Y_temp[0][-1]*data_bitcoin2015.loc[i-1+len(df_train), 'close'])\n",
        "                                      # ts_pred.append(data_bitcoin2015.loc[i-1+len(df_train), 'date'])\n",
        "                                    if np.any(np.isnan(Y_pred)): \n",
        "                                      #print('y pred has nan')\n",
        "                                      Y_pred = [np.nan_to_num(citem) for citem in Y_pred]  \n",
        "                                    if np.any(np.isnan(Y_real)):  \n",
        "                                      #print('y real has nan')\n",
        "                                      Y_real = [np.nan_to_num(citem) for citem in Y_real]\n",
        "                                    mserror = sklearn.metrics.mean_squared_error(Y_real, Y_pred)\n",
        "                                    # best_out_test = Y_pred_rel\n",
        "                                    corrrr = np.corrcoef(np.array(Y_real).reshape((len(Y_real))),np.array(Y_pred).reshape((len(Y_real))))[1,0]\n",
        "                                    #print(mserror)\n",
        "                                    if class_acc>best_acc:\n",
        "                                      temp_model= keras.models.clone_model(model)\n",
        "                                      # model_copy.build((None, 10)) # replace 10 with number of variables in input layer\n",
        "                                      # model_copy.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "                                      temp_model.set_weights(model.get_weights())\n",
        "                                      best_acc = class_acc\n",
        "                                      best_ts_pred = ts_pred\n",
        "                                      best_out_test = Y_pred_rel \n",
        "                                  else:\n",
        "                                    print('has nan') \n",
        "                                n_retry+=1\n",
        "                                \n",
        "                              cnt_ts = 0\n",
        "                              for ts_item in best_ts_pred:\n",
        "                                if not ts_item in list_time_stomps_lstm:\n",
        "                                  list_time_stomps_lstm.append(ts_item)\n",
        "                                  list_perdictions_lstm.append(best_out_test[cnt_ts][0])\n",
        "                                cnt_ts+=1\n",
        "                              \n",
        "                              model= keras.models.clone_model(temp_model)\n",
        "                              # model_copy.build((None, 10)) # replace 10 with number of variables in input layer\n",
        "                              # model_copy.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "                              model.set_weights(temp_model.get_weights())\n",
        "                              #print('classification_accuracy', best_acc)\n",
        "                          #   #print(len(df_use_norm))\n",
        "                          #   #print(len(pred_use))\n",
        "    \n",
        "                            if method == 'LSTM_self_Attention':\n",
        "                              best_acc = 0\n",
        "                              n_retry = 0\n",
        "                              \n",
        "                              while n_retry <4 and (n_retry >1 or best_acc <0.55):\n",
        "                                n_retry+=1\n",
        "                                model = LSTM_self_attention_net(X_train, Y_train, in_shape, ep =50, ver=0)\n",
        "    \n",
        "                                # Y_pred_rel = list()\n",
        "                                # Y_real_rel = list()\n",
        "                                # for i in range(len(X_use)):\n",
        "                                #   X_temp = np.array(X_use[i]).reshape(1, seq_size-1, num_of_features)\n",
        "                                #   Y_temp = model.predict(X_temp)\n",
        "                                #   Y_real_rel.append(Y_test[i])\n",
        "                                #   Y_pred_rel.append(Y_temp[0][-1])\n",
        "                                #   pred_use.append(Y_temp[0][-1])\n",
        "                                  \n",
        "                                  \n",
        "    \n",
        "                                Y_pred_rel = list()\n",
        "                                Y_real_rel = list()\n",
        "                                for i in range(len(X_test)):\n",
        "                                  X_temp = np.array(X_test[i]).reshape(1, seq_size-1, num_of_features)\n",
        "                                  Y_temp = model.predict(X_temp)\n",
        "                                  Y_real_rel.append(Y_test[i])\n",
        "                                  Y_pred_rel.append(Y_temp[0][-1])\n",
        "                              #     if Y_temp[0][-1]>1:\n",
        "                              #       buy_sell_vec.append(1)\n",
        "                              #     elif  Y_temp[0][-1]<1:\n",
        "                              #       buy_sell_vec.append(2)\n",
        "                              #     else:\n",
        "                              #       buy_sell_vec.append(0)\n",
        "                              #     if len(risk_sensitive_param)==1:\n",
        "                              #       buy_sell_vec_intensity.append(risk_sensitive_func(Y_temp[0][-1],risk_sensitive_param[0],risk_mode))\n",
        "                              #     if len(risk_sensitive_param)>1:\n",
        "                              #       buy_sell_vec_intensity.append(risk_sensitive_func(Y_temp[0][-1],risk_sensitive_param[0],risk_mode,risk_sensitive_param[1]))\n",
        "                                mserror_rel = sklearn.metrics.mean_squared_error(Y_real_rel, Y_pred_rel)\n",
        "                                corrrr_rel = np.corrcoef(np.array(Y_real_rel).reshape((len(Y_real_rel))),np.array(Y_pred_rel).reshape((len(Y_real_rel))))[1,0]\n",
        "                                class_acc = np.mean(((np.array(Y_real_rel)>1)&(np.array(Y_pred_rel)>1))|((np.array(Y_real_rel)<1)&(np.array(Y_pred_rel)<1)))\n",
        "                                #print('classification_accuracy', class_acc)\n",
        "                                ts_pred = list()\n",
        "                                Y_pred = list()\n",
        "                                Y_real = list()\n",
        "                                for i in range(len(X_test)):\n",
        "                                  X_temp = np.array(X_test[i]).reshape(1, seq_size-1, num_of_features)\n",
        "                                  Y_temp = model.predict(X_temp)\n",
        "                                  Y_real.append(Y_test[i]*data_bitcoin2015.loc[i-1+len(df_train), 'close'])\n",
        "                                  Y_pred.append(Y_temp[0][-1]*data_bitcoin2015.loc[i-1+len(df_train), 'close'])\n",
        "                                  ts_pred.append(data_bitcoin2015.loc[i-1+len(df_train), 'date'])\n",
        "                                mserror = sklearn.metrics.mean_squared_error(Y_real, Y_pred)\n",
        "                                corrrr = np.corrcoef(np.array(Y_real).reshape((len(Y_real))),np.array(Y_pred).reshape((len(Y_real))))[1,0]\n",
        "                                #print(mserror)\n",
        "                                if class_acc>best_acc:\n",
        "                                  best_ts_pred = ts_pred\n",
        "                                  best_out_test = Y_pred_rel\n",
        "                                  temp_model= keras.models.clone_model(model)\n",
        "                                  # model_copy.build((None, 10)) # replace 10 with number of variables in input layer\n",
        "                                  # model_copy.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "                                  temp_model.set_weights(model.get_weights())\n",
        "                                  best_acc = class_acc\n",
        "                              cnt_ts = 0\n",
        "                              for ts_item in best_ts_pred:\n",
        "                                if not ts_item in list_time_stomps_lstm_attention:\n",
        "                                  list_time_stomps_lstm_attention.append(ts_item)\n",
        "                                  list_perdictions_lstm_attention.append(best_out_test[cnt_ts][0])\n",
        "                                cnt_ts+=1\n",
        "                              model= keras.models.clone_model(temp_model)\n",
        "                              # model_copy.build((None, 10)) # replace 10 with number of variables in input layer\n",
        "                              # model_copy.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "                              model.set_weights(temp_model.get_weights())\n",
        "                              #print('classification_accuracy', best_acc)\n",
        "                            #print(len(df_use_norm))\n",
        "                            #print(len(pred_use))\n",
        "                            if method == 'prev_day':\n",
        "                              pred_use.append(1)\n",
        "                              class_acc = 0\n",
        "                              Y_pred_rel = list()\n",
        "                              Y_real_rel = list()\n",
        "                              for i in range(len(X_test)):\n",
        "                                buy_sell_vec.append(random.randint(0,2))\n",
        "                                # buy_sell_vec_intensity.append(risk_sensitive_func(random.random()*10-4,risk_sensitive_param[0],risk_mode))\n",
        "                                if len(risk_sensitive_param)==1:\n",
        "                                  buy_sell_vec_intensity.append(risk_sensitive_func(random.random()*10,risk_sensitive_param[0],risk_mode))\n",
        "                                if len(risk_sensitive_param)>1:\n",
        "                                  buy_sell_vec_intensity.append(risk_sensitive_func(random.random()*10,risk_sensitive_param[0],risk_mode,risk_sensitive_param[1]))\n",
        "                                Y_pred_rel.append(1)\n",
        "                                \n",
        "                                Y_real_rel.append(Y_test[i])\n",
        "                              Y_pred = list()\n",
        "                              Y_real = list()\n",
        "                              for i in range(len(X_test)):\n",
        "                                if len(Y_real) >0:\n",
        "                                  Y_pred.append(Y_real[-1])\n",
        "                                else:\n",
        "                                  Y_pred.append(1*data_bitcoin2015.loc[i-1+len(df_train), 'close'])\n",
        "                                Y_real.append(Y_test[i]*data_bitcoin2015.loc[i-1+len(df_train), 'close'])\n",
        "                              mserror = sklearn.metrics.mean_squared_error(Y_real, Y_pred)\n",
        "                              corrrr = np.corrcoef(np.array(Y_real).reshape((len(Y_real))),np.array(Y_pred).reshape((len(Y_real))))[1,0]\n",
        "                          df_predictions_hourly = pd.DataFrame()\n",
        "                          df_predictions_hourly['date'] = list_time_stomps_lstm\n",
        "                          df_predictions_hourly['Estimated_change_ratio_LSTM'] = list(np.array(list_perdictions_lstm))\n",
        "                          df_predictions_hourly.loc[:,'Estimated_change_ratio_LSTM_Attention'] = pd.Series(list(np.array(list_perdictions_lstm)))\n",
        "                          # df_predictions_hourly.to_csv('/content/drive/MyDrive/inference_daily_'+created_date+'.csv')                  \n",
        "                          # if profiting_interval:\n",
        "                          df_predictions_hourly.to_csv('/content/drive/MyDrive/inference_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_'+mode_dataset_strip+'_task_1'+'.csv')                \n",
        "                        \n",
        "              if mode_dataset_strip == 'remove_first_month':\n",
        "                start_date2 =start_date2+pd.DateOffset(days=round(30))\n",
        "              ender_date =ender_date+pd.DateOffset(days=round(30))\n",
        "              end_train_date =end_train_date+pd.DateOffset(days=round(30))\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            df_predictions_hourly = pd.DataFrame()\n",
        "            df_predictions_hourly['date'] = list_time_stomps_lstm\n",
        "            df_predictions_hourly['Estimated_change_ratio_LSTM'] = list(np.array(list_perdictions_lstm))\n",
        "            df_predictions_hourly.loc[:,'Estimated_change_ratio_LSTM_Attention'] = pd.Series(list(np.array(list_perdictions_lstm)))\n",
        "            # df_predictions_hourly.to_csv('/content/drive/MyDrive/inference_daily_'+created_date+'.csv')                  \n",
        "            # if profiting_interval:\n",
        "            df_predictions_hourly.to_csv('/content/drive/MyDrive/inference_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_'+mode_dataset_strip+'_task_1'+'.csv')                \n",
        "            # if not profiting_interval:\n",
        "            #   df_predictions_hourly.to_csv('/content/drive/MyDrive/inference_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_non_profiting'+'.csv')   \n",
        "        \n",
        "        \n",
        "        if do_trades_in_this_loop:\n",
        "          print('starting the trading')\n",
        "          if os.path.exists('/content/drive/MyDrive/inference_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_'+mode_dataset_strip+'_task_1'+'.csv'):\n",
        "            df_predictions_hourly = pd.read_csv('/content/drive/MyDrive/inference_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_'+mode_dataset_strip+'_task_1'+'.csv')                \n",
        "          else:\n",
        "            print('doesnt exisst')\n",
        "          # print(df_predictions_hourly.date)\n",
        "  \n",
        "          df = load_data(file_path, dataset_name+\".csv\")\n",
        "          \n",
        "          if dataset_type == 'daily':\n",
        "            df['date']= pd.to_datetime(df['date'],format='%Y-%m-%d', errors='coerce')\n",
        "          else:\n",
        "            df['date']= pd.to_datetime(df['date'],format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "          # print(df.date)\n",
        "          df = df.loc[df['date'].isin(list(df_predictions_hourly['date']))]\n",
        "          # df['date'] = df.loc[df['date'].isin(list(df_predictions_hourly['date']))]\n",
        "          df.reset_index()\n",
        "          df['close2'] = df['close']\n",
        "          data_bitcoin2015 = df.copy()\n",
        "          # data_bitcoin2015.remove('Unnamed: 0')\n",
        "          data_bitcoin2015 = data_bitcoin2015.reset_index(drop=True)\n",
        "          # data_bitcoin2015.remove('Unnamed: 0')\n",
        "          # print(data_bitcoin2015.date)\n",
        "          data_bitcoin2015_normalized = data_bitcoin2015.copy()\n",
        "          for col in data_bitcoin2015_normalized.columns:\n",
        "            data_bitcoin2015_normalized[col].fillna(method='ffill', inplace=True)\n",
        "          \n",
        "          num_rows = len(data_bitcoin2015_normalized)\n",
        "          columns = list(data_bitcoin2015_normalized.columns)\n",
        "          \n",
        "          dd = datetime.now()\n",
        "          created_date = str(dd.year)+'-'+str(dd.month)+'-'+str(dd.day)+' -H'+str(dd.hour)\n",
        "          \n",
        "          # if profiting_interval:\n",
        "          \n",
        "          # if not profiting_interval:\n",
        "          #   if os.path.exists('/content/drive/MyDrive/inference_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_profiting'+'.csv'):\n",
        "          #     df_predictions_hourly = pd.read_csv('/content/drive/MyDrive/inference_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_non_profiting'+'.csv')                \n",
        "          for use_loss_limit in [False, True]:  \n",
        "            def sell_with_loss_limit(ld_oc,btc_price_open,btc_price_close,loss_limit,profit_limit,use_loss_limit=use_loss_limit):\n",
        "              d_oc = btc_price_close-btc_price_open\n",
        "              if d_oc>0:\n",
        "                sell_price = btc_price_close\n",
        "              if d_oc<=0:\n",
        "                sell_price = max(btc_price_open-abs(d_oc)*loss_limit,btc_price_close)\n",
        "              # return min(btc_price_open,btc_price_close)\n",
        "              return btc_price_open\n",
        "              if not use_loss_limit:\n",
        "                return btc_price_close\n",
        "              else:\n",
        "                return sell_price\n",
        "  \n",
        "            def buy_with_loss_limit(ld_oc,btc_price_open,btc_price_close,loss_limit,profit_limit,use_loss_limit=use_loss_limit):\n",
        "              d_oc = btc_price_close-btc_price_open\n",
        "              if d_oc>0:\n",
        "                buy_price = min(btc_price_open+abs(d_oc)*loss_limit,btc_price_close)\n",
        "              if d_oc<=0:\n",
        "                buy_price = btc_price_close\n",
        "              return btc_price_open\n",
        "              if not use_loss_limit:\n",
        "                return btc_price_close\n",
        "              else:\n",
        "                return buy_price\n",
        "              # return max(btc_price_open,btc_price_close)\n",
        "              # return btc_price_close\n",
        "              # return buy_price\n",
        "            loss_bound = True\n",
        "            loss_bound = False\n",
        "            loss_limit = 0.01\n",
        "            import pickle\n",
        "            MAX_EPISODES = 3\n",
        "            import os\n",
        "            BATCH_SIZE = 32\n",
        "            num_runs = 10\n",
        "            Risk_modes = ['Thresholdrisk','Pragmatism','Threshold','RiskLevel']\n",
        "            method_list = ['LSTM','buy_at_first','random']\n",
        "            feature_engineering_method_list = ['Corr']#,'Granger']\n",
        "            risk_sensitive_params = [1,2,5,10,40]\n",
        "            corr_threshold_list = [0.8] #[0,0.2,0.4,0.6,0.7,0.75,0.8,0.85,9]\n",
        "            split_percentage_list = [0.9] # [0.75,0.8,0.85,0.9]\n",
        "            seq_size_list = [10]\n",
        "            normalise_list = [1]\n",
        "            days_after_to_predict_list = [1]\n",
        "            #,3,5,7]\n",
        "            max_mse = [200000,200000,200000,200000]\n",
        "            min_class_acc = [0,0,0,0]\n",
        "            has_runned_percentage_method = [[False,False,False],[False,False,False],[False,False,False],[False,False,False]]\n",
        "            DDQN_runned = [[False,False,False],[False,False,False],[False,False,False],[False,False,False]]\n",
        "            DDQN_LSTM_runned = [[False,False,False],[False,False,False],[False,False,False],[False,False,False]]\n",
        "            runned_before = 0\n",
        "            #  np.zeros((len(days_after_to_predict_list),len(split_percentage_list),len(method_list)))\n",
        "            df_results = pd.DataFrame()\n",
        "            # dataset_name = 'Crypto_Datasets/'+cbtc_daily'\n",
        "            # df_results  = pd.read_pickle('/content/drive/MyDrive/Mofid_Ether_infer/result_'+dataset_name+'.pkl')\n",
        "            # df_results.index= [i for i in range(len(df_results))]\n",
        "            # pd.set_option('display.max_rows', len(df_results))\n",
        "            num_r = 0 \n",
        "  \n",
        "            for method in method_list:\n",
        "              for risk_mode in Risk_modes:\n",
        "                risk_sensitive_params = get_risk_sensitive_params(risk_mode)\n",
        "                cnt_rsp = -1 \n",
        "                for risk_sensitive_param in risk_sensitive_params:\n",
        "                  cnt_rsp+=1\n",
        "                  #print('method = ',method)\n",
        "                  #print('risk_mode = ',risk_mode)\n",
        "                  last_dco = 1\n",
        "                  #print('risk_param = ',risk_sensitive_param)\n",
        "                  if method in ['buy_at_first']:\n",
        "                    profit = []\n",
        "                    profit_time_stomps = []\n",
        "                    trades = []\n",
        "  \n",
        "                    num_btc = 0\n",
        "                    num_dollar = 1000\n",
        "                    is_first = False\n",
        "                    for i in range(len(data_bitcoin2015)):\n",
        "                      current_btc_price = data_bitcoin2015.loc[i,'close']\n",
        "                      last_btc_price = data_bitcoin2015.loc[i,'open']\n",
        "                      current_time = str(data_bitcoin2015.loc[i,'date'])\n",
        "                      # if data_bitcoin2015.loc[i,'close']/data_bitcoin2015.loc[i,'Open']<0.99:\n",
        "                        # print(data_bitcoin2015.loc[i,'close']/data_bitcoin2015.loc[i,'Open'])\n",
        "                      # Get information from LSTM\n",
        "                      if not is_first:\n",
        "                        # print(data_bitcoin2015)\n",
        "                      #   print('use loss limit = ',use_loss_limit)\n",
        "                      #   print('current_time = ',current_time)\n",
        "                      #   print('buyed with price' + str(last_btc_price))\n",
        "                        is_first = True\n",
        "                        num_btc += (1-Transaction_cost_rate)*num_dollar*1/last_btc_price\n",
        "                        num_dollar = 0\n",
        "                        trades.append(1)\n",
        "                        profit.append(num_btc*last_btc_price+num_dollar-1000)\n",
        "                        profit_time_stomps.append(current_time)\n",
        "                      else:\n",
        "                        profit.append(num_btc*last_btc_price+num_dollar-1000)\n",
        "                        trades.append(0)\n",
        "                        profit_time_stomps.append(current_time)\n",
        "                      last_dco = data_bitcoin2015.loc[i,'close']-data_bitcoin2015.loc[i,'open']\n",
        "                  #   print(last_btc_price)\n",
        "                  #   print(profit[-1])\n",
        "                  if method in ['random']:\n",
        "                    profit = []\n",
        "                    profit_time_stomps = []\n",
        "                    trades = []\n",
        "                    num_btc = 0\n",
        "                    num_dollar = 1000\n",
        "                    # is_first = False\n",
        "                    for i in range(len(data_bitcoin2015)):\n",
        "                      current_btc_price = data_bitcoin2015.loc[i,'close']\n",
        "                      last_btc_price = data_bitcoin2015.loc[i,'open']\n",
        "                      current_time = str(data_bitcoin2015.loc[i,'date'])\n",
        "                      # Get information from LSTM\n",
        "                      buy_sell = random.randint(0,2)\n",
        "                      if buy_sell==0:\n",
        "                        profit.append(num_btc*last_btc_price+num_dollar-1000)\n",
        "                        profit_time_stomps.append(current_time)\n",
        "                        trades.append(0)\n",
        "                      if buy_sell==1:\n",
        "                        num_btc += (1-Transaction_cost_rate)*num_dollar/last_btc_price\n",
        "                        num_dollar -= num_dollar\n",
        "                        trades.append(1)\n",
        "                        profit.append(num_btc*last_btc_price+num_dollar-1000)\n",
        "                        profit_time_stomps.append(current_time)\n",
        "                      \n",
        "                      if buy_sell==2:\n",
        "                        num_dollar += (1-Transaction_cost_rate)*num_btc*last_btc_price\n",
        "                        num_btc -=num_btc\n",
        "                        trades.append(-1)\n",
        "                        profit.append(num_btc*last_btc_price+num_dollar-1000)\n",
        "                        profit_time_stomps.append(current_time)\n",
        "                  if method in ['LSTM','LSTM_self_Attention']:\n",
        "                    profit = []\n",
        "                    profit_time_stomps = []\n",
        "                    trades = []\n",
        "                    buffer = [1,1,1]\n",
        "                    num_btc = 0\n",
        "                    last_action = 'sell'\n",
        "                    last_price = 0\n",
        "                    num_dollar = 1000\n",
        "                    for i in range(len(data_bitcoin2015)):\n",
        "                      current_btc_price = data_bitcoin2015.loc[i,'close']\n",
        "                      last_btc_price = data_bitcoin2015.loc[i,'open']\n",
        "                      current_time = str(data_bitcoin2015.loc[i,'date'])\n",
        "                      btc_price_open = data_bitcoin2015.loc[i,'open']\n",
        "                      btc_price_high = data_bitcoin2015.loc[i,'high']\n",
        "                      btc_price_low = data_bitcoin2015.loc[i,'low']\n",
        "                      btc_price_close = data_bitcoin2015.loc[i,'close']\n",
        "                      # Get information from LSTM\n",
        "                      if dataset_type == 'daily':\n",
        "                        get_item = df_predictions_hourly.loc[df_predictions_hourly['date'].astype('string') == re.search(r'\\d{4}-\\d{2}-\\d{2}', str(data_bitcoin2015.loc[i,'date'])).group()]\n",
        "                      else:\n",
        "                        get_item = df_predictions_hourly.loc[df_predictions_hourly['date'].astype('string') == re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', str(data_bitcoin2015.loc[i,'date'])).group()]\n",
        "                        \n",
        "                      acted_ = False\n",
        "                      # if len(get_item)>0:\n",
        "                      #   #print(1)\n",
        "                      #   #print(re.search(r'\\d{4}-\\d{2}-\\d{2}', str(data_bitcoin2015.loc[i,'date'])).group())\n",
        "                      if last_action == 'buy':\n",
        "                        if use_loss_limit:\n",
        "                          if dont_sell_inside_candle:\n",
        "                            if data_bitcoin2015.loc[i,'open']/last_price < 0.975:\n",
        "                              acted_ = True\n",
        "                              num_dollar += (1-Transaction_cost_rate)*num_btc*1*data_bitcoin2015.loc[i,'open']\n",
        "                              # num_dollar += num_btc*1*max(last_btc_price,current_btc_price)\n",
        "                              num_btc -=num_btc*1\n",
        "                              last_action = 'sell'\n",
        "                              trades.append(-1)\n",
        "                              profit.append(num_btc*data_bitcoin2015.loc[i,'open']+num_dollar-1000)\n",
        "                              profit_time_stomps.append(current_time)\n",
        "                          else:\n",
        "                            if data_bitcoin2015.loc[i,'close']/last_price < 0.975 and btc_price_low<last_price*0.975<btc_price_high:\n",
        "                              acted_ = True\n",
        "                              num_dollar += (1-Transaction_cost_rate)*num_btc*1*last_price*0.975\n",
        "                              # num_dollar += num_btc*1*max(last_btc_price,current_btc_price)\n",
        "                              num_btc -=num_btc*1\n",
        "                              last_action = 'sell'\n",
        "                              trades.append(-1)\n",
        "                              profit.append(num_btc*last_btc_price+num_dollar-1000)\n",
        "                              profit_time_stomps.append(current_time)\n",
        "                          \n",
        "                      if not acted_:\n",
        "                        if len(get_item)>0:\n",
        "                          # print('1')\n",
        "                          Estimated_change_ratio_LSTM = float(get_item['Estimated_change_ratio_LSTM'])\n",
        "                          buy_sell = 0\n",
        "                          buffer[0] = buffer[1]\n",
        "                          buffer[1] = buffer[2]\n",
        "                          buffer[2] = Estimated_change_ratio_LSTM\n",
        "                          last_price = current_btc_price\n",
        "                          if Estimated_change_ratio_LSTM>1:\n",
        "                            last_action = 'buy'\n",
        "                            buy_sell = 1\n",
        "                            # print('buy')\n",
        "                          elif  Estimated_change_ratio_LSTM<1:\n",
        "                            last_action = 'sell'\n",
        "                            buy_sell = 2\n",
        "                          else:\n",
        "                            buy_sell = 0\n",
        "                          if len(risk_sensitive_param)==1:\n",
        "                            # print('intens')\n",
        "                            buy_sell_vec_intensity = risk_sensitive_func(Estimated_change_ratio_LSTM,risk_sensitive_param[0],risk_mode)\n",
        "                          \n",
        "                          if len(risk_sensitive_param)>1:\n",
        "                            buy_sell_vec_intensity = risk_sensitive_func(Estimated_change_ratio_LSTM,risk_sensitive_param[0],risk_mode,risk_sensitive_param[1])\n",
        "                          \n",
        "                          if do_the_new_idea:\n",
        "                              # buy_sell_vec_intensity = 1\n",
        "                              nn_sheeb1 = buffer[2] - buffer[1]\n",
        "                              nn_sheeb2 = buffer[1] - buffer[0]\n",
        "                              avg = np.mean(buffer)\n",
        "                              \n",
        "                              if avg >1 and nn_sheeb1<nn_sheeb2:\n",
        "                                  buy_sell = 1\n",
        "                              elif avg <1 and nn_sheeb2<nn_sheeb1:\n",
        "                                  buy_sell = 2\n",
        "                              else:\n",
        "                                  buy_sell = 0\n",
        "                                                          \n",
        "                          if buy_sell==0:\n",
        "                            profit.append(num_btc*last_btc_price+num_dollar-1000)\n",
        "                            profit_time_stomps.append(current_time)\n",
        "                            trades.append(0)\n",
        "                          if buy_sell==1:\n",
        "                            num_btc += (1-Transaction_cost_rate)*num_dollar*buy_sell_vec_intensity/buy_with_loss_limit(last_dco,btc_price_open,btc_price_close,loss_limit,use_loss_limit)\n",
        "                            num_dollar -= num_dollar*buy_sell_vec_intensity\n",
        "                            trades.append(buy_sell_vec_intensity)\n",
        "                            profit.append(num_btc*last_btc_price+num_dollar-1000)\n",
        "                            profit_time_stomps.append(current_time)\n",
        "                          if buy_sell==2:\n",
        "                            num_dollar += (1-Transaction_cost_rate)*num_btc*buy_sell_vec_intensity*sell_with_loss_limit(last_dco,btc_price_open,btc_price_close,loss_limit,use_loss_limit)\n",
        "                            # max(btc_price_open,btc_price_close)\n",
        "                            # buy_with_loss_limit(last_dco,btc_price_open,btc_price_close,loss_limit)\n",
        "                            num_btc -=num_btc*buy_sell_vec_intensity\n",
        "                            trades.append(-buy_sell_vec_intensity)\n",
        "                            profit.append(num_btc*last_btc_price+num_dollar-1000)\n",
        "                            profit_time_stomps.append(current_time)\n",
        "                        else:\n",
        "                          profit.append(num_btc*last_btc_price+num_dollar-1000)\n",
        "                          profit_time_stomps.append(current_time)\n",
        "                          trades.append(0)\n",
        "                      last_dco = data_bitcoin2015.loc[i,'close']-data_bitcoin2015.loc[i,'open']\n",
        "                  #print(profit[-1])\n",
        "                  df_results['date'] = profit_time_stomps\n",
        "                  df_results[method + '_'+str(risk_mode)+'_'+str(cnt_rsp)+'_profit'] = profit\n",
        "                  df_results[method + '_'+str(risk_mode)+'_'+str(cnt_rsp)+'_tradings'] = trades\n",
        "                  # df_results['best_acc'] = [best_acc for i in range(len(profit_time_stomps))]\n",
        "  \n",
        "            if use_loss_limit:\n",
        "              # if profiting_interval:\n",
        "              df_results.to_csv('/content/drive/MyDrive/long_term_simulation_with_loss_bound_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_'+mode_dataset_strip+'_task_1'+'.csv')\n",
        "              # if not profiting_interval:\n",
        "              #   df_results.to_csv('/content/drive/MyDrive/long_term_simulation_with_loss_bound_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_non_profiting'+'.csv')    \n",
        "            else:\n",
        "              # if profiting_interval:\n",
        "              df_results.to_csv('/content/drive/MyDrive/long_term_simulation_without_loss_bound_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_'+mode_dataset_strip+'_task_1'+'.csv')\n",
        "              # if not profiting_interval:\n",
        "              #   df_results.to_csv('/content/drive/MyDrive/long_term_simulation_without_loss_bound_'+dataset_type+'_'+crypto_type+'_'+str(observe_delay)+'_non_profiting'+'.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "round  1\n",
            "mode_dataset_strip  remove_first_month\n",
            "dataset_type  daily\n",
            "observe_delay  \n",
            "resume incomplete process\n",
            "end_train_time =  2021-10-13 00:00:00\n",
            "ender_time =  2021-11-13 00:00:00\n",
            "starting the predictions\n",
            "starting the trading\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-966ee6409069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    795\u001b[0m                       \u001b[0;31m# Get information from LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                       \u001b[0;32mif\u001b[0m \u001b[0mdataset_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'daily'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                         \u001b[0mget_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_predictions_hourly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_predictions_hourly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\d{4}-\\d{2}-\\d{2}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bitcoin2015\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m                       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                         \u001b[0mget_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_predictions_hourly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_predictions_hourly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bitcoin2015\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;31m# caller is responsible for ensuring non-None axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m         \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(index, key)\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \u001b[0;31m# key may contain nan elements, check_array_indexer needs bool array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2401\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_array_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexers.py\u001b[0m in \u001b[0;36mcheck_array_indexer\u001b[0;34m(array, indexer)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_bool_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36mto_numpy\u001b[0;34m(self, dtype, copy, na_value, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;31m# error: Too many arguments for \"to_numpy\" of \"ExtensionArray\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             return self.array.to_numpy(  # type: ignore[call-arg]\n\u001b[0;32m--> 514\u001b[0;31m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m             )\n\u001b[1;32m    516\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/arrays/masked.py\u001b[0m in \u001b[0;36mto_numpy\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;31m# \"Type[object]\", variable has type \"Union[str, dtype[Any], None]\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hasna\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             if (\n\u001b[1;32m    288\u001b[0m                 \u001b[0;32mnot\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/arrays/masked.py\u001b[0m in \u001b[0;36m_hasna\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# error: Incompatible return value type (got \"bool_\", expected \"bool\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[return-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_any\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXfRca9VcEhT",
        "outputId": "75388b8f-5f4a-4ef3-e3a3-dfe1ed90c7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jVplc6IRb-ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAYhZw7TqbQJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}